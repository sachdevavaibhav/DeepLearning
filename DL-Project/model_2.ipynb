{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "558132a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26eb2ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained Inception-ResNet-V2 model\n",
    "base_model = tf.keras.applications.InceptionResNetV2(weights='imagenet', include_top=False, input_shape=(250,250,3))\n",
    "\n",
    "# Freeze the pre-trained weights\n",
    "base_model.trainable = False\n",
    "\n",
    "# Define the input shape\n",
    "input_shape = (250, 250, 3)\n",
    "\n",
    "# Define the input layer\n",
    "inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "\n",
    "# Pass the input layer through the pre-trained model\n",
    "features = base_model(inputs)\n",
    "\n",
    "# Add a dense layer for multi-class classification\n",
    "# outputs = tf.keras.layers.Dense(units=8, activation='sigmoid')(features)\n",
    "\n",
    "outputs = tf.keras.layers.Flatten()(features)\n",
    "outputs = tf.keras.layers.Dense(units=8, activation='sigmoid')(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87a82410",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa\n",
    "accuracy = tf.keras.metrics.Accuracy()\n",
    "precision = tf.keras.metrics.Precision()\n",
    "recall = tf.keras.metrics.Recall()\n",
    "f1_score = tfa.metrics.F1Score(num_classes=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "465e6446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy', precision, recall, f1_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb595890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 250, 250, 3)]     0         \n",
      "                                                                 \n",
      " inception_resnet_v2 (Functi  (None, 6, 6, 1536)       54336736  \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 55296)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 8)                 442376    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 54,779,112\n",
      "Trainable params: 442,376\n",
      "Non-trainable params: 54,336,736\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0519635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image data and one-hot encoded multi-label data\n",
    "# Load the data from the excel sheet into a pandas dataframe\n",
    "# Load both left and right eye dataset\n",
    "base_path = 'C:/Users/User/DeepLearning/DL-Project'\n",
    "left_df = pd.read_excel(f'{base_path}/updated_left_eye.xlsx')\n",
    "right_df = pd.read_excel(f'{base_path}/updated_right_eye.xlsx')\n",
    "\n",
    "#concating both dataframes\n",
    "df = pd.concat([left_df, right_df], axis=0)\n",
    "\n",
    "\n",
    "# Split the data into two arrays, one for the image paths and one for the labels\n",
    "images_base_path = 'E:/odir-dataset/ODIR-5K/ODIR-5K/cropped_training_images/'\n",
    "image_paths = images_base_path + df[\"Fundus\"].values\n",
    "labels = df.iloc[:, 3:].values\n",
    "\n",
    "# Convert the lists to tensors\n",
    "image_paths = tf.constant(image_paths)\n",
    "labels = tf.constant(labels)\n",
    "\n",
    "# Create a dataset from the image paths and labels\n",
    "dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a052d9",
   "metadata": {},
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60e53a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 250, 250, 3)]     0         \n",
      "                                                                 \n",
      " inception_resnet_v2 (Functi  (None, 6, 6, 1536)       54336736  \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 55296)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 8)                 442376    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 54,779,112\n",
      "Trainable params: 442,376\n",
      "Non-trainable params: 54,336,736\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2123cef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 250, 250, 3)]     0         \n",
      "                                                                 \n",
      " inception_resnet_v2 (Functi  (None, 6, 6, 1536)       54336736  \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 55296)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 8)                 442376    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 54,779,112\n",
      "Trainable params: 442,376\n",
      "Non-trainable params: 54,336,736\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "628307f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the images and preprocess them\n",
    "def load_and_preprocess_image(file_path, label):\n",
    "    image = tf.io.read_file(file_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, [250, 250])\n",
    "    image = tf.keras.applications.inception_resnet_v2.preprocess_input(image)\n",
    "    return image, label\n",
    "\n",
    "# Apply the load_and_preprocess_image function to the dataset\n",
    "dataset = dataset.map(load_and_preprocess_image)\n",
    "\n",
    "# Shuffle the data and batch it\n",
    "batch_size = 32\n",
    "dataset = dataset.shuffle(buffer_size=len(image_paths)).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bce1df36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset element_spec=(TensorSpec(shape=(None, 250, 250, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 8), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7e72b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(dataset, train_split, test_split, val_split):\n",
    "    # Calculate the size of each split\n",
    "    dataset_size = dataset.reduce(tf.constant(0, dtype=tf.int64), lambda acc, _: acc + 1).numpy()\n",
    "    train_size = int(dataset_size * train_split)\n",
    "    val_size = int(dataset_size * val_split)\n",
    "    test_size = int(dataset_size * test_split)\n",
    "\n",
    "    # Shuffle the elements of the dataset randomly\n",
    "    dataset = dataset.shuffle(buffer_size=dataset_size, seed=42)\n",
    "\n",
    "    # Split the dataset into train, validation, and test sets\n",
    "    train_dataset = dataset.take(train_size)\n",
    "    remaining_dataset = dataset.skip(train_size)\n",
    "    val_dataset = remaining_dataset.take(val_size)\n",
    "    test_dataset = remaining_dataset.skip(val_size)\n",
    "    return train_dataset, test_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7aecb7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets\n",
    "train_dataset, test_dataset, val_dataset = split_data(dataset, train_split=0.7, test_split=0.15, val_split=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc3d6410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "152/152 [==============================] - 1026s 6s/step - loss: 7.8826 - accuracy: 0.4421 - precision: 0.1379 - recall: 0.9877 - f1_score: 0.2124 - val_loss: 4.9353 - val_accuracy: 0.4407 - val_precision: 0.1341 - val_recall: 1.0000 - val_f1_score: 0.2136\n",
      "Epoch 2/10\n",
      "152/152 [==============================] - 1003s 6s/step - loss: 4.5284 - accuracy: 0.4485 - precision: 0.1340 - recall: 1.0000 - f1_score: 0.2117 - val_loss: 3.2648 - val_accuracy: 0.4301 - val_precision: 0.1344 - val_recall: 1.0000 - val_f1_score: 0.2145\n",
      "Epoch 3/10\n",
      "152/152 [==============================] - 907s 6s/step - loss: 4.1962 - accuracy: 0.4484 - precision: 0.1341 - recall: 1.0000 - f1_score: 0.2124 - val_loss: 5.8221 - val_accuracy: 0.4651 - val_precision: 0.1324 - val_recall: 1.0000 - val_f1_score: 0.2091\n",
      "Epoch 4/10\n",
      "152/152 [==============================] - 1038s 6s/step - loss: 4.7766 - accuracy: 0.4393 - precision: 0.1339 - recall: 1.0000 - f1_score: 0.2127 - val_loss: 5.5564 - val_accuracy: 0.4476 - val_precision: 0.1333 - val_recall: 1.0000 - val_f1_score: 0.2106\n",
      "Epoch 5/10\n",
      "152/152 [==============================] - 1136s 7s/step - loss: 3.5150 - accuracy: 0.4370 - precision: 0.1341 - recall: 1.0000 - f1_score: 0.2134 - val_loss: 2.3887 - val_accuracy: 0.4403 - val_precision: 0.1342 - val_recall: 1.0000 - val_f1_score: 0.2134\n",
      "Epoch 6/10\n",
      "152/152 [==============================] - 1224s 7s/step - loss: 3.6396 - accuracy: 0.4382 - precision: 0.1341 - recall: 1.0000 - f1_score: 0.2129 - val_loss: 2.2765 - val_accuracy: 0.4550 - val_precision: 0.1345 - val_recall: 1.0000 - val_f1_score: 0.2124\n",
      "Epoch 7/10\n",
      "152/152 [==============================] - 1079s 6s/step - loss: 3.1384 - accuracy: 0.4393 - precision: 0.1340 - recall: 1.0000 - f1_score: 0.2128 - val_loss: 2.9217 - val_accuracy: 0.4833 - val_precision: 0.1334 - val_recall: 1.0000 - val_f1_score: 0.2089\n",
      "Epoch 8/10\n",
      "152/152 [==============================] - 1285s 8s/step - loss: 2.9918 - accuracy: 0.4436 - precision: 0.1338 - recall: 1.0000 - f1_score: 0.2121 - val_loss: 3.5385 - val_accuracy: 0.4476 - val_precision: 0.1350 - val_recall: 1.0000 - val_f1_score: 0.2140\n",
      "Epoch 9/10\n",
      "152/152 [==============================] - 1275s 7s/step - loss: 2.4251 - accuracy: 0.4452 - precision: 0.1340 - recall: 1.0000 - f1_score: 0.2125 - val_loss: 1.8144 - val_accuracy: 0.4384 - val_precision: 0.1334 - val_recall: 1.0000 - val_f1_score: 0.2116\n",
      "Epoch 10/10\n",
      "152/152 [==============================] - 1206s 7s/step - loss: 3.0075 - accuracy: 0.4439 - precision: 0.1345 - recall: 1.0000 - f1_score: 0.2130 - val_loss: 2.6109 - val_accuracy: 0.4246 - val_precision: 0.1353 - val_recall: 1.0000 - val_f1_score: 0.2161\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history=model.fit(train_dataset, epochs=10, batch_size=batch_size, validation_data=test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3058bd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('C:/Users/User/DeepLearning/DL-Project/models/model_2.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
